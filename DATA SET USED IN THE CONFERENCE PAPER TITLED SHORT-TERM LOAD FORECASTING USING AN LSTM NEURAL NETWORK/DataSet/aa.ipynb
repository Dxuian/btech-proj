# %%
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
# Load datasets
load_data = pd.read_excel('C:\\Users\death\\Desktop\\btech proj\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\DataSet\\LoadTexasERCOT.xlsx')
weather_data1 = pd.read_excel('C:\\Users\\death\Desktop\\btech proj\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\DataSet\\WeatherStation1.xlsx')
weather_data2 = pd.read_excel('C:\\Users\\death\Desktop\\btech proj\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\DataSet\\WeatherStation2.xlsx')
weather_data3 = pd.read_excel('C:\\Users\\death\Desktop\\btech proj\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\DataSet\\WeatherStation3.xlsx')



# %%
weather_data1.head(5)

# %%
weather_data3.head(5)

# %%

weather_data2.head(5)

# %%
import pandas as pd

# Assuming weather_data1 is already defined and contains the necessary data

# Print column names to verify
print("Columns in weather_data1:", weather_data1.columns)

# Step 1: Preprocessing
# Convert to Datetime
weather_data1['DateTime'] = pd.to_datetime(weather_data1[['Year', 'Month', 'Day', 'Hour' ]])

# Drop the original columns
weather_data1.drop(columns=['Year', 'Month', 'Day', 'Hour' ], inplace=True)

# Display the first few rows to verify the changes
weather_data1.head()

# %%
weather_data3['DateTime'] = pd.to_datetime(weather_data2[['Year', 'Month', 'Day', 'Hour']])
weather_data3.drop(columns=['Year', 'Month', 'Day', 'Hour'], inplace=True)
weather_data2['DateTime'] = pd.to_datetime(weather_data2[['Year', 'Month', 'Day', 'Hour']])
weather_data2.drop(columns=['Year', 'Month', 'Day', 'Hour'], inplace=True)

# %%
merged_data = pd.merge(load_data, weather_data1, left_on='Hour_End', right_on='DateTime')
merged_data = pd.merge(merged_data, weather_data3, left_on='Hour_End', right_on='DateTime', suffixes=('_w1', '_w3'))
merged_data = pd.merge(merged_data, weather_data2, left_on='Hour_End', right_on='DateTime', suffixes=('_w1', '_w3', '_w2'))
merged_data.drop(columns=['DateTime_w1',  'DateTime_w3','Minute_w1','Minute_w3'], inplace=True)

# %%
merged_data.head(5)
# merged_data.drop(columns=['DateTime_w1', 'DateTime_w2', 'DateTime_w3', 'DateTime'], inplace=True)

# %% [markdown]
# ## Creating Input features
# 

# %%
import holidays
import pandas as pd

# Assuming 'merged_data' DataFrame has 'Hour_End' column as datetime object
# Example: merged_data['Hour_End'] = pd.to_datetime(merged_data['Hour_End'])

# Step 1: Add 'Time of Day Index' (Hour of the day)
merged_data['TimeOfDayIndex'] = merged_data['Hour_End'].dt.hour

# Step 2: Add 'Day of the Week Index' (0 = Monday, 6 = Sunday)
merged_data['DayOfWeekIndex'] = merged_data['Hour_End'].dt.dayofweek

# Step 3: Add 'Holiday Flag' using the 'holidays' library for U.S. and Texas holidays
us_holidays = holidays.US(years=merged_data['Hour_End'].dt.year.unique(), state='TX')
merged_data['HolidayFlag'] = merged_data['Hour_End'].apply(lambda x: 1 if x in us_holidays else 0)

# Step 4: One-Hot Encode 'TimeOfDayIndex', 'DayOfWeekIndex', and 'HolidayFlag'
merged_data = pd.get_dummies(merged_data, columns=['TimeOfDayIndex', 'DayOfWeekIndex', 'HolidayFlag'], drop_first=True)
pd.set_option('display.max_columns', None)

# Drop 'DateTime' column if it exists
if 'DateTime' in merged_data.columns:
    merged_data.drop(['DateTime'], axis=1, inplace=True)

# Display the first few rows to verify the one-hot encoded features
print(merged_data.head())

# Count the number of rows where 'HolidayFlag_1' is 1 and 0
holiday_flag_1_count = merged_data['HolidayFlag_1'].sum()
holiday_flag_0_count = len(merged_data) - holiday_flag_1_count

print(f"Number of rows where HolidayFlag_1 is 1: {holiday_flag_1_count}")
print(f"Number of rows where HolidayFlag_1 is 0: {holiday_flag_0_count}")
print(merged_data.dtypes)


# %%


# %%
# import pandas as pd

# # Assuming scaled_data is already defined and contains the necessary data

# def print_unique_values(data, num_values=7):
#     for column in data.columns:
#         unique_values = data[column].unique()[:num_values]
#         print(f"Column: {column}")
#         print(f"Unique Values: {unique_values}")
#         print()

# # Call the function to print unique values
# print_unique_values(scaled_data)

# %%


# %%
# X

# %%
# X.shape , y.shape

# %%
# y

# %% [markdown]
# ## Train/test split

# %%

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# %% [markdown]
# ## Building the l ass tm model

# %%
# Split the data into training and testing sets
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import tensorflow as tf
# Assuming merged_data is already defined and contains the necessary data

# Exclude the datetime and 'ERCOT' columns before scaling
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_features = scaler.fit_transform(merged_data.drop(columns=['Hour_End']))

# Convert scaled features back to DataFrame and re-include the 'Hour_End' and 'ERCOT' columns
scaled_data = pd.DataFrame(scaled_features, columns=merged_data.columns.drop(['Hour_End']))
# scaled_data['ERCOT'] = merged_data['ERCOT'].values

# scaled_data['Hour_End'] = merged_data['Hour_End'].values

#print dtype of all columns
print(scaled_data.dtypes)
scaled_data.head(5)

# Ensure all data is of type float64
scaled_data = scaled_data.astype(np.float64)

def create_sequences(data, time_steps):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data.iloc[i:i + time_steps].drop(columns=['ERCOT']).values)
        y.append(data.iloc[i + time_steps]['ERCOT'])
    return np.array(X), np.array(y)

# Assuming scaled_data is already defined and contains the necessary data
time_steps = 18  # Example time steps
X, y = create_sequences(scaled_data, time_steps)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Ensure all data is of type float32
X_train = X_train.astype(np.float64)
X_test = X_test.astype(np.float64)
y_train = y_train.astype(np.float64)
y_test = y_test.astype(np.float64)

# Define the model
model = Sequential()
model.add(LSTM(55, return_sequences=True, input_shape=(time_steps, X_train.shape[2])))
model.add(LSTM(50))
model.add(Dense(1))  # Output layer for load prediction

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_test, y_test))

# Make predictions
# Make predictions
# Make predictions
# Make predictions
train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test)

# Inverse transform the predictions back to original scale
# Ensure the concatenated array has the correct shape
# Drop the last column from X_train and X_test to match the scaler's expected input shape
# Use the original scaler's feature range to inverse transform only the target variable

# Create a new scaler for the target variable 'ERCOT'
target_scaler = MinMaxScaler(feature_range=(0, 1))
target_scaler.min_, target_scaler.scale_ = scaler.min_[-1], scaler.scale_[-1]

# Inverse transform the predictions
scaled_train_predictions = target_scaler.inverse_transform(train_predictions)
scaled_test_predictions = target_scaler.inverse_transform(test_predictions)
scaled_y_train = target_scaler.inverse_transform(y_train.reshape(-1, 1))
scaled_y_test = target_scaler.inverse_transform(y_test.reshape(-1, 1))

# Evaluate the performance
# Evaluate the performance

# Evaluate the performance
# Evaluate the performance

# %%
from sklearn.metrics import r2_score

# Calculate MAE and RMSE
train_mae = mean_absolute_error(scaled_y_train, scaled_train_predictions)
test_mae = mean_absolute_error(scaled_y_test, scaled_test_predictions)
train_rmse = np.sqrt(mean_squared_error(scaled_y_train, scaled_train_predictions))
test_rmse = np.sqrt(mean_squared_error(scaled_y_test, scaled_test_predictions))

# Calculate R² score
train_r2 = r2_score(scaled_y_train, scaled_train_predictions)
test_r2 = r2_score(scaled_y_test, scaled_test_predictions)

# Print the results
print(f'Training MAE: {train_mae}, RMSE: {train_rmse}, R²: {train_r2}')
print(f'Testing MAE: {test_mae}, RMSE: {test_rmse}, R²: {test_r2}')

# Step 8: Plot the results
plt.figure(figsize=(14, 7))
plt.plot(scaled_y_test, label='Actual Load')
plt.plot(scaled_test_predictions, label='Predicted Load')
plt.title('Actual vs Predicted Load (Testing Set)')
plt.xlabel('Time')
plt.ylabel('Load (MW)')
plt.legend()
plt.show()


