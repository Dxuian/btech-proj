{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\death\\AppData\\Local\\Temp\\ipykernel_18876\\1115631666.py:15: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  load_data = pd.read_excel('C:\\\\Users\\death\\\\Desktop\\\\btech proj\\\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\\\DataSet\\\\LoadTexasERCOT.xlsx')\n",
      "C:\\Users\\death\\AppData\\Local\\Temp\\ipykernel_18876\\1115631666.py:16: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  weather_data1 = pd.read_excel('C:\\\\Users\\\\death\\Desktop\\\\btech proj\\\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\\\DataSet\\\\WeatherStation1.xlsx')\n",
      "C:\\Users\\death\\AppData\\Local\\Temp\\ipykernel_18876\\1115631666.py:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  weather_data2 = pd.read_excel('C:\\\\Users\\\\death\\Desktop\\\\btech proj\\\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\\\DataSet\\\\WeatherStation2.xlsx')\n",
      "C:\\Users\\death\\AppData\\Local\\Temp\\ipykernel_18876\\1115631666.py:18: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  weather_data3 = pd.read_excel('C:\\\\Users\\\\death\\Desktop\\\\btech proj\\\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\\\DataSet\\\\WeatherStation3.xlsx')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Year  Month  Day  Hour  Minute  Relative Humidity  Temperature  Pressure\n",
       " 0  2012      1    1     0      30              39.19            6       950\n",
       " 1  2012      1    1     1      30              44.45            5       950\n",
       " 2  2012      1    1     2      30              53.94            4       950\n",
       " 3  2012      1    1     3      30              63.77            2       960\n",
       " 4  2012      1    1     4      30              71.93            2       960,\n",
       "    Year  Month  Day  Hour  Minute  Relative Humidity  Temperature  Pressure\n",
       " 0  2012      1    1     0      30              63.98            8       950\n",
       " 1  2012      1    1     1      30              49.59            6       950\n",
       " 2  2012      1    1     2      30              46.28            4       950\n",
       " 3  2012      1    1     3      30              52.89            3       960\n",
       " 4  2012      1    1     4      30              62.81            3       960,\n",
       "    Year  Month  Day  Hour  Minute  Relative Humidity  Temperature  Pressure\n",
       " 0  2012      1    1     0      30              42.80            6       960\n",
       " 1  2012      1    1     1      30              56.15            5       960\n",
       " 2  2012      1    1     2      30              63.72            3       960\n",
       " 3  2012      1    1     3      30              69.80            2       960\n",
       " 4  2012      1    1     4      30              74.92            1       970)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# Load datasets\n",
    "load_data = pd.read_excel('C:\\\\Users\\death\\\\Desktop\\\\btech proj\\\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\\\DataSet\\\\LoadTexasERCOT.xlsx')\n",
    "weather_data1 = pd.read_excel('C:\\\\Users\\\\death\\Desktop\\\\btech proj\\\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\\\DataSet\\\\WeatherStation1.xlsx')\n",
    "weather_data2 = pd.read_excel('C:\\\\Users\\\\death\\Desktop\\\\btech proj\\\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\\\DataSet\\\\WeatherStation2.xlsx')\n",
    "weather_data3 = pd.read_excel('C:\\\\Users\\\\death\\Desktop\\\\btech proj\\\\DATA SET USED IN THE CONFERENCE PAPER TITLED SHORT-TERM LOAD FORECASTING USING AN LSTM NEURAL NETWORK\\\\DataSet\\\\WeatherStation3.xlsx')\n",
    "\n",
    "target = 'WEST'\n",
    "weather_data1.head(5) , weather_data2.head(5) , weather_data3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in weather_data1: Index(['Year', 'Month', 'Day', 'Hour', 'Minute', 'Relative Humidity',\n",
      "       'Temperature', 'Pressure'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35064, 5), (35064, 5), (35064, 5), (35064, 2))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming weather_data1 is already defined and contains the necessary data\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"Columns in weather_data1:\", weather_data1.columns)\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "# Convert to Datetime\n",
    "weather_data1['DateTime'] = pd.to_datetime(weather_data1[['Year', 'Month', 'Day', 'Hour' ]])\n",
    "\n",
    "# Drop the original columns\n",
    "weather_data1.drop(columns=['Year', 'Month', 'Day', 'Hour' ], inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "weather_data3['DateTime'] = pd.to_datetime(weather_data2[['Year', 'Month', 'Day', 'Hour']])\n",
    "weather_data3.drop(columns=['Year', 'Month', 'Day', 'Hour'], inplace=True)\n",
    "weather_data2['DateTime'] = pd.to_datetime(weather_data2[['Year', 'Month', 'Day', 'Hour']])\n",
    "weather_data2.drop(columns=['Year', 'Month', 'Day', 'Hour'], inplace=True)\n",
    "load_data.drop(columns=[col for col in load_data.columns if col not in ['Hour_End', target]], inplace=True)\n",
    "weather_data1.shape  , weather_data2.shape , weather_data3.shape , load_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming weather_data1, weather_data2, and weather_data3 are your DataFrames\n",
    "\n",
    "# Ensure DateTime columns are in datetime format\n",
    "\n",
    "# Concatenate the three dataframes on top of each other\n",
    "combined_df = pd.concat([weather_data1, weather_data2, weather_data3])\n",
    "\n",
    "# Group by the DateTime column and calculate the mean for all other columns\n",
    "result_df = combined_df.groupby('DateTime').mean().reset_index()\n",
    "\n",
    "# Now, result_df contains the DateTime column and averaged values for all other columns\n",
    "#now merge result df on DateTime with load data on Hour_End \n",
    "final_df = pd.merge(result_df, load_data, left_on='DateTime', right_on='Hour_End')\n",
    "result_df = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11687, 7)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35064, 5), (35064, 5), (35064, 5), (11687, 7))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data1.shape , weather_data2.shape , weather_data3.shape , result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data2 = pd.merge(load_data, weather_data3, left_on='Hour_End', right_on='DateTime')\n",
    "# merged_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data3 = pd.merge(load_data, weather_data2, left_on='Hour_End', right_on='DateTime' )\n",
    "# merged_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Relative Humidity</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Hour_End</th>\n",
       "      <th>WEST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01 03:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>62.153333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>960.000000</td>\n",
       "      <td>2012-01-01 03:00:00</td>\n",
       "      <td>840.902848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-01 06:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>76.233333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>963.333333</td>\n",
       "      <td>2012-01-01 06:00:00</td>\n",
       "      <td>892.341115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-01 09:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>46.803333</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>963.333333</td>\n",
       "      <td>2012-01-01 09:00:00</td>\n",
       "      <td>1009.202331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-01 12:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.700000</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>963.333333</td>\n",
       "      <td>2012-01-01 12:00:00</td>\n",
       "      <td>964.065395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-01 15:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.413333</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>963.333333</td>\n",
       "      <td>2012-01-01 15:00:00</td>\n",
       "      <td>895.595594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>2015-12-31 09:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>93.536667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>953.333333</td>\n",
       "      <td>2015-12-31 09:00:00</td>\n",
       "      <td>1366.707739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>2015-12-31 12:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>84.633333</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>953.333333</td>\n",
       "      <td>2015-12-31 12:00:00</td>\n",
       "      <td>1309.101953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>2015-12-31 15:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.333333</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>953.333333</td>\n",
       "      <td>2015-12-31 15:00:00</td>\n",
       "      <td>1183.585632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>2015-12-31 18:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>72.553333</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>953.333333</td>\n",
       "      <td>2015-12-31 18:00:00</td>\n",
       "      <td>1229.206403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11686</th>\n",
       "      <td>2015-12-31 21:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>78.080000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>953.333333</td>\n",
       "      <td>2015-12-31 21:00:00</td>\n",
       "      <td>1234.948091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11687 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 DateTime  Minute  Relative Humidity  Temperature    Pressure  \\\n",
       "0     2012-01-01 03:00:00    30.0          62.153333     2.333333  960.000000   \n",
       "1     2012-01-01 06:00:00    30.0          76.233333     0.333333  963.333333   \n",
       "2     2012-01-01 09:00:00    30.0          46.803333     6.333333  963.333333   \n",
       "3     2012-01-01 12:00:00    30.0          26.700000    12.333333  963.333333   \n",
       "4     2012-01-01 15:00:00    30.0          25.413333    12.000000  963.333333   \n",
       "...                   ...     ...                ...          ...         ...   \n",
       "11682 2015-12-31 09:00:00    30.0          93.536667     3.666667  953.333333   \n",
       "11683 2015-12-31 12:00:00    30.0          84.633333     6.333333  953.333333   \n",
       "11684 2015-12-31 15:00:00    30.0          80.333333     6.000000  953.333333   \n",
       "11685 2015-12-31 18:00:00    30.0          72.553333    -1.333333  953.333333   \n",
       "11686 2015-12-31 21:00:00    30.0          78.080000    -2.000000  953.333333   \n",
       "\n",
       "                 Hour_End         WEST  \n",
       "0     2012-01-01 03:00:00   840.902848  \n",
       "1     2012-01-01 06:00:00   892.341115  \n",
       "2     2012-01-01 09:00:00  1009.202331  \n",
       "3     2012-01-01 12:00:00   964.065395  \n",
       "4     2012-01-01 15:00:00   895.595594  \n",
       "...                   ...          ...  \n",
       "11682 2015-12-31 09:00:00  1366.707739  \n",
       "11683 2015-12-31 12:00:00  1309.101953  \n",
       "11684 2015-12-31 15:00:00  1183.585632  \n",
       "11685 2015-12-31 18:00:00  1229.206403  \n",
       "11686 2015-12-31 21:00:00  1234.948091  \n",
       "\n",
       "[11687 rows x 7 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stack all 3 mergered dataframes\n",
    "# final_data = pd.concat([merged_data, merged_data2, merged_data3], axis=0)\n",
    "# merged_data  = final_data\n",
    "# #drop the DateTime column\n",
    "# merged_data.drop(columns=['DateTime'], inplace=True)\n",
    "\n",
    "merged_data =  result_df\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where HolidayFlag_1 is 1: 623\n",
      "Number of rows where HolidayFlag_1 is 0: 11064\n"
     ]
    }
   ],
   "source": [
    "import holidays\n",
    "\n",
    "us_holidays = holidays.US(years=merged_data['Hour_End'].dt.year.unique(), state='TX')\n",
    "merged_data['HolidayFlag'] = merged_data['Hour_End'].apply(lambda x: 1 if x in us_holidays else 0)\n",
    "holiday_flag_1_count = merged_data['HolidayFlag'].sum()\n",
    "holiday_flag_0_count = len(merged_data) - holiday_flag_1_count\n",
    "\n",
    "print(f\"Number of rows where HolidayFlag_1 is 1: {holiday_flag_1_count}\")\n",
    "print(f\"Number of rows where HolidayFlag_1 is 0: {holiday_flag_0_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Input features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Minute  Relative Humidity  Temperature    Pressure            Hour_End  \\\n",
      "0    30.0          62.153333     2.333333  960.000000 2012-01-01 03:00:00   \n",
      "1    30.0          76.233333     0.333333  963.333333 2012-01-01 06:00:00   \n",
      "2    30.0          46.803333     6.333333  963.333333 2012-01-01 09:00:00   \n",
      "3    30.0          26.700000    12.333333  963.333333 2012-01-01 12:00:00   \n",
      "4    30.0          25.413333    12.000000  963.333333 2012-01-01 15:00:00   \n",
      "\n",
      "          WEST  HolidayFlag  TimeOfDay_0  TimeOfDay_1  TimeOfDay_2  ...  \\\n",
      "0   840.902848            1            0            0            0  ...   \n",
      "1   892.341115            1            0            0            0  ...   \n",
      "2  1009.202331            1            0            0            0  ...   \n",
      "3   964.065395            1            0            0            0  ...   \n",
      "4   895.595594            1            0            0            0  ...   \n",
      "\n",
      "   TimeOfDay_21  TimeOfDay_22  TimeOfDay_23  DayOfWeek_0  DayOfWeek_1  \\\n",
      "0             0             0             0            0            0   \n",
      "1             0             0             0            0            0   \n",
      "2             0             0             0            0            0   \n",
      "3             0             0             0            0            0   \n",
      "4             0             0             0            0            0   \n",
      "\n",
      "   DayOfWeek_2  DayOfWeek_3  DayOfWeek_4  DayOfWeek_5  DayOfWeek_6  \n",
      "0            0            0            0            0            1  \n",
      "1            0            0            0            0            1  \n",
      "2            0            0            0            0            1  \n",
      "3            0            0            0            0            1  \n",
      "4            0            0            0            0            1  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "Column names are:\n",
      "Minute, Relative Humidity, Temperature, Pressure, Hour_End, WEST, HolidayFlag, TimeOfDay_0, TimeOfDay_1, TimeOfDay_2, TimeOfDay_3, TimeOfDay_4, TimeOfDay_5, TimeOfDay_6, TimeOfDay_7, TimeOfDay_8, TimeOfDay_9, TimeOfDay_10, TimeOfDay_11, TimeOfDay_12, TimeOfDay_13, TimeOfDay_14, TimeOfDay_15, TimeOfDay_16, TimeOfDay_17, TimeOfDay_18, TimeOfDay_19, TimeOfDay_20, TimeOfDay_21, TimeOfDay_22, TimeOfDay_23, DayOfWeek_0, DayOfWeek_1, DayOfWeek_2, DayOfWeek_3, DayOfWeek_4, DayOfWeek_5, DayOfWeek_6\n",
      "shape of the data is: (11687, 38)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_data' DataFrame has 'Hour_End' column as datetime object\n",
    "# Example: merged_data['Hour_End'] = pd.to_datetime(merged_data['Hour_End'])\n",
    "\n",
    "# Step 1: Add 'Time of Day Index' (Hour of the day)\n",
    "merged_data['TimeOfDayIndex'] = merged_data['Hour_End'].dt.hour\n",
    "\n",
    "# Step 2: Add 'Day of the Week Index' (0 = Monday, 6 = Sunday)\n",
    "merged_data['DayOfWeekIndex'] = merged_data['Hour_End'].dt.dayofweek\n",
    "\n",
    "# Step 3: Add 'Holiday Flag' using the 'holidays' library for U.S. and Texas holidays\n",
    "\n",
    "# Step 4: Manually create 24 columns for 'TimeOfDayIndex' and 7 columns for 'DayOfWeekIndex'\n",
    "for hour in range(24):\n",
    "    merged_data[f'TimeOfDay_{hour}'] = (merged_data['TimeOfDayIndex'] == hour).astype(int)\n",
    "\n",
    "for day in range(7):\n",
    "    merged_data[f'DayOfWeek_{day}'] = (merged_data['DayOfWeekIndex'] == day).astype(int)\n",
    "\n",
    "# Drop the original 'TimeOfDayIndex' and 'DayOfWeekIndex' columns\n",
    "merged_data.drop(columns=['TimeOfDayIndex', 'DayOfWeekIndex'], inplace=True)\n",
    "\n",
    "# Drop 'DateTime' column if it exists\n",
    "if 'DateTime' in merged_data.columns:\n",
    "    merged_data.drop(['DateTime'], axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the manually created features\n",
    "print(merged_data.head())\n",
    "\n",
    "# Count the number of rows where 'HolidayFlag' is 1 and 0\n",
    "# Display the column names to verify the new columns\n",
    "print('Column names are:\\n' + ', '.join(merged_data.columns))\n",
    "print('shape of the data is:', merged_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the l ass tm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Exclude the datetime and target columns before scaling\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = feature_scaler.fit_transform(merged_data.drop(columns=['Hour_End', target]))\n",
    "\n",
    "# Scale the target column separately\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_target = target_scaler.fit_transform(merged_data[[target]])\n",
    "\n",
    "# Convert scaled features back to DataFrame and re-include the 'Hour_End' and scaled target columns\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=merged_data.columns.drop(['Hour_End', target]))\n",
    "scaled_data[target] = scaled_target\n",
    "scaled_data['Hour_End'] = merged_data['Hour_End'].values\n",
    "\n",
    "# Print dtype of all columns\n",
    "print(scaled_data.dtypes)\n",
    "scaled_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    # Ensure hour_end is a datetime column\n",
    "    df['Hour_End'] = pd.to_datetime(df['Hour_End'])\n",
    "    \n",
    "    # Split the data\n",
    "    train = df[df['Hour_End'].dt.year != 2015]\n",
    "    test = df[df['Hour_End'].dt.year == 2015]\n",
    "    \n",
    "    return train, test\n",
    "tr , te = split_data(scaled_data)\n",
    "print(tr.shape)\n",
    "#drop the Hour_End column from tr and te\n",
    "tr.drop(columns=['Hour_End'], inplace=True)\n",
    "te.drop(columns=['Hour_End'], inplace=True)\n",
    "tr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(te.shape)\n",
    "te.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_data = scaled_data.astype(np.float64)\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, time_steps, target):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data.iloc[i:i + time_steps].drop(columns=[target]).values)\n",
    "        y.append(data.iloc[i + time_steps][target])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Assuming scaled_data is already defined and contains the necessary data\n",
    "time_steps = 18  # Example time steps\n",
    "target = target\n",
    "X_train, y_train = create_sequences(tr, time_steps, target)\n",
    "X_test,  y_test = create_sequences(te, time_steps, target)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure all data is of type float64\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "y_train = y_train.astype(np.float64)\n",
    "y_test = y_test.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, target_scaler, max_epochs=50, batch_size=32):\n",
    "    # Initialize variables to store the best results\n",
    "    best_train_mae = float('inf')\n",
    "    best_test_mae = float('inf')\n",
    "    best_train_rmse = float('inf')\n",
    "    best_test_rmse = float('inf')\n",
    "    best_train_r2 = float('-inf')\n",
    "    best_test_r2 = float('-inf')\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    best_test_predictions = None\n",
    "    best_train_predictions = None\n",
    "    # Loop through the number of epochs\n",
    "    for epoch in [5,10,15,20,30,]:\n",
    "        # Define the model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(55, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(50))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1))  # Output layer for load prediction\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # Train the model for the current number of epochs\n",
    "        model.fit(X_train, y_train, epochs=epoch, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = model.predict(X_train)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        \n",
    "        # Inverse transform the predictions and target variables\n",
    "        train_predictions = target_scaler.inverse_transform(train_predictions)\n",
    "        test_predictions = target_scaler.inverse_transform(test_predictions)\n",
    "        y_train_inv = target_scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mae = mean_absolute_error(y_train_inv, train_predictions)\n",
    "        test_mae = mean_absolute_error(y_test_inv, test_predictions)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_inv, train_predictions))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test_inv, test_predictions))\n",
    "        train_r2 = r2_score(y_train_inv, train_predictions)\n",
    "        test_r2 = r2_score(y_test_inv, test_predictions)\n",
    "        \n",
    "        # Update best metrics and model if applicable\n",
    "        if train_mae < best_train_mae and test_mae < best_test_mae:\n",
    "            best_train_mae = train_mae\n",
    "            best_test_mae = test_mae\n",
    "            best_train_rmse = train_rmse\n",
    "            best_test_rmse = test_rmse\n",
    "            best_train_r2 = train_r2\n",
    "            best_test_r2 = test_r2\n",
    "            best_model = model\n",
    "            best_epoch = epoch\n",
    "            best_train_predictions = train_predictions\n",
    "            best_test_predictions = test_predictions\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train MAE = {train_mae:.4f}, Test MAE = {test_mae:.4f}\")\n",
    "        print(f\"Epoch {epoch}: Train RMSE = {train_rmse:.4f}, Test RMSE = {test_rmse:.4f}\")\n",
    "        print(f\"Epoch {epoch}: Train R² = {train_r2:.4f}, Test R² = {test_r2:.4f}\")\n",
    "\n",
    "    # Final evaluation metrics\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Train MAE: {best_train_mae:.4f}\")\n",
    "    print(f\"Best Test MAE: {best_test_mae:.4f}\")\n",
    "    print(f\"Best Train RMSE: {best_train_rmse:.4f}\")\n",
    "    print(f\"Best Test RMSE: {best_test_rmse:.4f}\")\n",
    "    print(f\"Best Train R²: {best_train_r2:.4f}\")\n",
    "    print(f\"Best Test R²: {best_test_r2:.4f}\")\n",
    "\n",
    "    # Return the best model and the metrics\n",
    "    return best_model, best_train_mae, best_test_mae, best_train_rmse, best_test_rmse, best_train_r2, best_test_r2 , best_train_predictions, best_test_predictions\n",
    "\n",
    "# Example usage:\n",
    "model, train_mae, test_mae, train_rmse, test_rmse, train_r2, test_r2 , train_predictions , test_predictions = train_and_evaluate_model(X_train, y_train, X_test, y_test, target_scaler, max_epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(55, return_sequences=True, input_shape=(time_steps, X_train.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1))  # Output layer for load prediction\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=10,batch_size=32 ,validation_data=(X_test, y_test))\n",
    "\n",
    "# # Make predictions\n",
    "# train_predictions = model.predict(X_train)\n",
    "# test_predictions = model.predict(X_test)\n",
    "\n",
    "# # Inverse transform the predictions and target variables\n",
    "# train_predictions = target_scaler.inverse_transform(train_predictions)\n",
    "# test_predictions = target_scaler.inverse_transform(test_predictions)\n",
    "# y_train = target_scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "# y_test = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "# train_predictions , y_train , test_predictions , y_test\n",
    "# # Evaluate model\n",
    "# train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "# test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "# train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "# test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "# train_r2 = r2_score(y_train, train_predictions)\n",
    "# test_r2 = r2_score(y_test, test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(f'Training MAE: {train_mae}, RMSE: {train_rmse}, R²: {train_r2}')\n",
    "print(f'Testing MAE: {test_mae}, RMSE: {test_rmse}, R²: {test_r2}')\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 18))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(y_test, label='Actual Load', color='blue')\n",
    "plt.plot(test_predictions, label='Predicted Load', color='orange')\n",
    "plt.title('Actual vs Predicted Load (Testing Set)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load (MW)')\n",
    "plt.legend()\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(y_test, label='Actual Load', color='blue')\n",
    "plt.title('Actual Load (Testing Set)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load (MW)')\n",
    "plt.legend()\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(test_predictions, label='Predicted Load', color='orange')\n",
    "plt.title('Predicted Load (Testing Set)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load (MW)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Exclude the datetime and target columns before scaling\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaled_features = scaler.fit_transform(merged_data.drop(columns=['Hour_End', target]))\n",
    "\n",
    "# # Convert scaled features back to DataFrame and re-include the 'Hour_End' and target columns\n",
    "# scaled_data = pd.DataFrame(scaled_features, columns=merged_data.columns.drop(['Hour_End', target]))\n",
    "# scaled_data[target] = merged_data[target].values\n",
    "# scaled_data['Hour_End'] = merged_data['Hour_End'].values\n",
    "\n",
    "# # Print dtype of all columns\n",
    "# print(scaled_data.dtypes)\n",
    "# scaled_data.head(5)\n",
    "\n",
    "# # Ensure all data is of type float64\n",
    "# scaled_data = scaled_data.astype(np.float64)\n",
    "\n",
    "# # Function to create sequences\n",
    "# def create_sequences(data, time_steps, target):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(data) - time_steps):\n",
    "#         X.append(data.iloc[i:i + time_steps].drop(columns=[target]).values)\n",
    "#         y.append(data.iloc[i + time_steps][target])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # Assuming scaled_data is already defined and contains the necessary data\n",
    "# time_steps = 18  # Example time steps\n",
    "# X, y = create_sequences(scaled_data, time_steps, target)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# # Ensure all data is of type float64\n",
    "# X_train = X_train.astype(np.float64)\n",
    "# X_test = X_test.astype(np.float64)\n",
    "# y_train = y_train.astype(np.float64)\n",
    "# y_test = y_test.astype(np.float64)\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(55, return_sequences=True, input_shape=(time_steps, X_train.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1))  # Output layer for load prediction\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# # Make predictions\n",
    "# train_predictions = model.predict(X_train)\n",
    "# test_predictions = model.predict(X_test)\n",
    "\n",
    "# # Evaluate model\n",
    "# train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "# test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "# train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "# test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "# train_r2 = r2_score(y_train, train_predictions)\n",
    "# test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# print(f'Training MAE: {train_mae}, RMSE: {train_rmse}, R²: {train_r2}')\n",
    "# print(f'Testing MAE: {test_mae}, RMSE: {test_rmse}, R²: {test_r2}')\n",
    "\n",
    "# # Plotting the results\n",
    "# plt.figure(figsize=(14, 18))\n",
    "# plt.subplot(3, 1, 1)\n",
    "# plt.plot(y_test, label='Actual Load', color='blue')\n",
    "# plt.plot(test_predictions, label='Predicted Load', color='orange')\n",
    "# plt.title('Actual vs Predicted Load (Testing Set)')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Load (MW)')\n",
    "# plt.legend()\n",
    "# plt.subplot(3, 1, 2)\n",
    "# plt.plot(y_test, label='Actual Load', color='blue')\n",
    "# plt.title('Actual Load (Testing Set)')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Load (MW)')\n",
    "# plt.legend()\n",
    "# plt.subplot(3, 1, 3)\n",
    "# plt.plot(test_predictions, label='Predicted Load', color='orange')\n",
    "# plt.title('Predicted Load (Testing Set)')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Load (MW)')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 8: Plot the results in separate subplots\n",
    "# plt.figure(figsize=(14, 18))\n",
    "\n",
    "# # Plot overlapping actual and predicted load\n",
    "# plt.subplot(3, 1, 1)\n",
    "# plt.plot(scaled_y_test, label='Actual Load', color='blue')\n",
    "# plt.plot(scaled_test_predictions, label='Predicted Load', color='orange')\n",
    "# plt.title('Actual vs Predicted Load (Testing Set)')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Load (MW)')\n",
    "# plt.legend()\n",
    "\n",
    "# # Plot actual load\n",
    "# plt.subplot(3, 1, 2)\n",
    "# plt.plot(scaled_y_test, label='Actual Load', color='blue')\n",
    "# plt.title('Actual Load (Testing Set)')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Load (MW)')\n",
    "# plt.legend()\n",
    "\n",
    "# # Plot predicted load\n",
    "# plt.subplot(3, 1, 3)\n",
    "# plt.plot(scaled_test_predictions, label='Predicted Load', color='orange')\n",
    "# plt.title('Predicted Load (Testing Set)')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Load (MW)')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
